import os
import asyncio
from pydantic import SecretStr
from openhands.sdk import LLM, Agent, Conversation, Event, MessageEvent
from openhands.sdk.llm import content_to_str

class HostAgent:
    def __init__(self):
        api_key = os.getenv("LLM_API_KEY")
        if not api_key:
            # In a real scenario, we might want to handle this more gracefully
            # but for now, we'll assume it's provided.
            raise ValueError("LLM_API_KEY environment variable is not set.")
        
        self.llm = LLM(
            model=os.getenv("LLM_MODEL", "anthropic/claude-sonnet-4-5-20250929"),
            api_key=SecretStr(api_key),
            base_url=os.getenv("LLM_BASE_URL", None),
        )
        
        # No tools for now as requested. We'll add them in future iterations.
        self.agent = Agent(
            llm=self.llm,
            tools=[],
        )

    async def generate_response(self, context: str) -> str:
        """
        Generates a response on behalf of the Airbnb host based on the provided context.
        """
        loop = asyncio.get_running_loop()
        
        responses = []
        
        def callback(event: Event):
            # We only care about messages from the agent
            if isinstance(event, MessageEvent) and event.source == "agent":
                text_parts = content_to_str(event.llm_message.content)
                if text_parts:
                    responses.append("".join(text_parts))

        # Initialize conversation in the current workspace
        conversation = Conversation(
            agent=self.agent,
            callbacks=[callback],
            workspace=os.getcwd(),
        )

        # Construct the prompt for the agent
        # We explicitly tell the agent its role and provide the context.
        prompt = f"""
You are an Airbnb host assistant. Your goal is to help the host respond to guest inquiries professionally, warmly, and helpfully.

Below is the conversation history and context between the host and the guest:
---
{context}
---

Please generate a suitable response to the guest on behalf of the host. 
Only provide the response text itself, without any extra commentary or meta-talk.
"""
        
        # Run the conversation. Since conversation.run() is blocking, 
        # we run it in an executor to avoid blocking the FastAPI event loop.
        await loop.run_in_executor(None, lambda: self._run_conversation(conversation, prompt))
        
        if responses:
            # Return the last message generated by the agent
            return responses[-1].strip()
        
        return "I'm sorry, I couldn't generate a response at this time."

    def _run_conversation(self, conversation: Conversation, prompt: str):
        conversation.send_message(prompt)
        conversation.run()

# Singleton instance for the agent
_host_agent_instance = None

def get_host_agent():
    global _host_agent_instance
    if _host_agent_instance is None:
        _host_agent_instance = HostAgent()
    return _host_agent_instance

async def generate_agent_response(context: str) -> str:
    """
    Entry point for generating an agent response.
    This matches the expected interface for replacing mock_llm_response.
    """
    agent = get_host_agent()
    return await agent.generate_response(context)

if __name__ == "__main__":
    # Simple test script
    async def test():
        test_context = "Guest: Hi, is the apartment available this weekend?\nHost: Let me check."
        print("Generating response...")
        response = await generate_agent_response(test_context)
        print(f"Response: {response}")

    if os.getenv("LLM_API_KEY"):
        asyncio.run(test())
    else:
        print("Skipping test: LLM_API_KEY not set.")
